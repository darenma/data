{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cfc24d80c2cf2c24",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Adaboost\n",
    "For this exercise you will implement AdaBoost from scratch and applied it to a spam dataset. You will be classifying data into spam and not spam. You can call DecisionTreeClassifier from sklearn (with default max_depth=1) to learn your base classifiers.\n",
    "\n",
    "Here is how you train a decision tree classifier with weights.\n",
    "\n",
    "`\n",
    "h = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "h.fit(X, Y, sample_weight=w)\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3478d607a536190b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c56cb727222d9503",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# accuracy computation\n",
    "# this data is not highly imbalance so accuracy is ok\n",
    "def accuracy(y, pred):\n",
    "    return np.sum(y == pred) / float(len(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1eaf818528c8d676",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_spambase_data(filename):\n",
    "    \"\"\" Given a filename return X and Y numpy arrays\n",
    "\n",
    "    X is of size number of rows x num_features\n",
    "    Y is an array of size the number of rows\n",
    "    Y is the last element of each row. (Convert 0 to -1)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "   \n",
    "\n",
    "    ### END SOLUTION\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-73926460e1f70f54",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_test = np.array([1., -1., 1., 1., -1., -1., 1., 1., 1., -1.])\n",
    "X, Y = parse_spambase_data(\"tiny.spam.train\")\n",
    "for i in range(len(y_test)): assert(y_test[i] == Y[i])\n",
    "n, m = X.shape\n",
    "assert(n == 10)\n",
    "assert(m == 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-befcece7be9c6839",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def adaboost(X, y, num_iter, max_depth=1):\n",
    "    \"\"\"Given an numpy matrix X, a array y and num_iter return trees and weights \n",
    "   \n",
    "    Input: X, y, num_iter\n",
    "    Outputs: array of trees from DecisionTreeClassifier\n",
    "             trees_weights array of floats\n",
    "    Assumes y is {-1, 1}\n",
    "    \"\"\"\n",
    "    trees = []\n",
    "    trees_weights = [] \n",
    "    N, _ = X.shape\n",
    "    d = np.ones(N) / N\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return trees, trees_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4dc8edbf0e11fab8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X, Y = parse_spambase_data(\"tiny.spam.train\")\n",
    "trees, weights = adaboost(X, Y, 2)\n",
    "y_hat_0 = trees[0].predict(X)\n",
    "assert(len(trees) == 2)\n",
    "assert(len(weights) == 2)\n",
    "assert(isinstance(trees[0], DecisionTreeClassifier))\n",
    "assert(np.array_equal(y_hat_0[:5], [1.,-1.,1, 1, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8702186fedbd58e1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat_0 = trees[0].predict(X)\n",
    "assert(np.array_equal(y_hat_0[:5], [1.,-1.,1, 1, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50ad4a5c81e7c016",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def adaboost_predict(X, trees, trees_weights):\n",
    "    \"\"\"Given X, trees and weights predict Y\n",
    "    \"\"\"\n",
    "    # X input, y output\n",
    "    N, _ =  X.shape\n",
    "    y = np.zeros(N)\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7f28f00061f7b5a5",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[0, -1], [1, 0], [-1, 0]])\n",
    "y = np.array([-1, 1, 1])\n",
    "trees, weights = adaboost(x, y, 1)\n",
    "pred = adaboost_predict(x, trees, weights)\n",
    "assert(np.array_equal(pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-89126a5a7f8f0e1b",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9111\n",
      "Test Accuracy 0.9190\n"
     ]
    }
   ],
   "source": [
    "X, Y = parse_spambase_data(\"spambase.train\")\n",
    "X_test, Y_test = parse_spambase_data(\"spambase.test\")\n",
    "trees, trees_weights = adaboost(X, Y, 10)\n",
    "Yhat = adaboost_predict(X, trees, trees_weights)\n",
    "Yhat_test = adaboost_predict(X_test, trees, trees_weights)\n",
    "    \n",
    "acc_test = accuracy(Y_test, Yhat_test)\n",
    "acc_train = accuracy(Y, Yhat)\n",
    "print(\"Train Accuracy %.4f\" % acc_train)\n",
    "print(\"Test Accuracy %.4f\" % acc_test)\n",
    "assert(np.around(acc_train, decimals=4)==0.9111)\n",
    "assert(np.around(acc_test, decimals=4)==0.9190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting for regression with MSE loss\n",
    "For this exercise you will implement a version of gradient boosting from scratch and applied it to predict rental prices. You can call DecisionTreeRegressor from sklearn to learn your base classifiers.\n",
    " \n",
    "`tree = DecisionTreeRegressor(max_depth=max_depth, random_state=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = np.loadtxt(\"rent-ideal.csv\", delimiter=\",\", skiprows=1)\n",
    "    y = dataset[:, -1]\n",
    "    X = dataset[:, 0:- 1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-158a37c458437dee",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def gradient_boosting_mse(X, y, num_iter, max_depth=1, nu=0.1):\n",
    "    \"\"\"Given X, a array y and num_iter return y_mean and trees \n",
    "   \n",
    "    Input: X, y, num_iter\n",
    "           max_depth\n",
    "           nu (is the shinkage)\n",
    "    Outputs:y_mean, array of trees from DecisionTreeRegression\n",
    "    \"\"\"\n",
    "    trees = []\n",
    "    N, _ = X.shape\n",
    "    y_mean = np.mean(y)\n",
    "    fm = y_mean\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return y_mean, trees   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8ff3f16fc953ead4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_predict(X, trees, y_mean,  nu=0.1):\n",
    "    \"\"\"Given X, trees, y_mean predict y_hat\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f6f368de93b63c1c",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = load_dataset()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=3)\n",
    "\n",
    "y_mean, trees = gradient_boosting_mse(X_train, y_train, 300, max_depth=7, nu=0.1)\n",
    "assert(np.around(y_mean, decimals=4)==3434.7185)\n",
    "y_hat_train = gradient_boosting_predict(X_train, trees, y_mean, nu=0.1)\n",
    "assert(np.around(r2_score(y_train, y_hat_train), decimals=4)== 0.8993) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-62746a63ebdd7798",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat = gradient_boosting_predict(X_val, trees, y_mean, nu=0.1)\n",
    "assert(np.around(r2_score(y_val, y_hat), decimals=4)== 0.8399)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
